{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/liavnadam/Deep-Dream/blob/main/SAM2_Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dvo_Vlxg7bz",
        "outputId": "9548eb68-a424-4283-af0e-2d8d23e7fa8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "HOME = '/content/drive/MyDrive'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mzqmgk3KfpWM",
        "outputId": "b14dbc65-8a41-4967-af19-734358ac8540"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/segment-anything-2\n",
            "Obtaining file:///content/drive/MyDrive/segment-anything-2\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from SAM-2==1.0) (2.4.0+cu121)\n",
            "Requirement already satisfied: torchvision>=0.18.1 in /usr/local/lib/python3.10/dist-packages (from SAM-2==1.0) (0.19.0+cu121)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.10/dist-packages (from SAM-2==1.0) (1.26.4)\n",
            "Requirement already satisfied: tqdm>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from SAM-2==1.0) (4.66.5)\n",
            "Collecting hydra-core>=1.3.2 (from SAM-2==1.0)\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting iopath>=0.1.10 (from SAM-2==1.0)\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pillow>=9.4.0 in /usr/local/lib/python3.10/dist-packages (from SAM-2==1.0) (9.4.0)\n",
            "Collecting omegaconf<2.4,>=2.2 (from hydra-core>=1.3.2->SAM-2==1.0)\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from hydra-core>=1.3.2->SAM-2==1.0)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from hydra-core>=1.3.2->SAM-2==1.0) (24.1)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from iopath>=0.1.10->SAM-2==1.0) (4.12.2)\n",
            "Collecting portalocker (from iopath>=0.1.10->SAM-2==1.0)\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.1->SAM-2==1.0) (3.15.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.1->SAM-2==1.0) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.1->SAM-2==1.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.1->SAM-2==1.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.1->SAM-2==1.0) (2024.6.1)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.4,>=2.2->hydra-core>=1.3.2->SAM-2==1.0) (6.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.3.1->SAM-2==1.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.3.1->SAM-2==1.0) (1.3.0)\n",
            "Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Building wheels for collected packages: SAM-2, antlr4-python3-runtime, iopath\n",
            "  Building editable for SAM-2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for SAM-2: filename=SAM_2-1.0-0.editable-cp310-cp310-linux_x86_64.whl size=12331 sha256=dea4eceeaa7d97463ad4aa42b8df2447d18f769be6dfd2cafe02be65474a91d0\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-n8gbtpzd/wheels/ff/ac/4f/5de9ebab829cbd53b42df7e4854be4767b7ccfb9b422e18387\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=f4d640e66d84aac974a51cf26cd4086aa6efcf45cf4dd8eb8ba262fee72c877f\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31529 sha256=5ffc5e8fe182c90513ee314b48c12d18e9f268ae7afd765758c35d88716cc71b\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
            "Successfully built SAM-2 antlr4-python3-runtime iopath\n",
            "Installing collected packages: antlr4-python3-runtime, portalocker, omegaconf, iopath, hydra-core, SAM-2\n",
            "Successfully installed SAM-2-1.0 antlr4-python3-runtime-4.9.3 hydra-core-1.3.2 iopath-0.1.10 omegaconf-2.3.0 portalocker-2.10.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              },
              "id": "83119b01de61401eaa4a2e86c7a748dc"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/segment-anything-2\n",
        "#installing segment-anything-2\n",
        "!pip install -e .\n",
        "\n",
        "!mkdir -p {HOME}/checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1dTc7PIg7b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f5e3905-b53c-4707-85b8-0577bfdb5d16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Flask-Cors\n",
            "  Downloading Flask_Cors-5.0.0-py2.py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: Flask>=0.9 in /usr/local/lib/python3.10/dist-packages (from Flask-Cors) (2.2.5)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.9->Flask-Cors) (3.0.4)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.9->Flask-Cors) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.9->Flask-Cors) (2.2.0)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.9->Flask-Cors) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->Flask>=0.9->Flask-Cors) (2.1.5)\n",
            "Downloading Flask_Cors-5.0.0-py2.py3-none-any.whl (14 kB)\n",
            "Installing collected packages: Flask-Cors\n",
            "Successfully installed Flask-Cors-5.0.0\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install Flask-Cors\n",
        "!pip install pyngrok\n",
        "\n",
        "import sys\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "import base64\n",
        "import random\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "from pyngrok import ngrok\n",
        "import threading\n",
        "import traceback\n",
        "import matplotlib.pyplot as plt\n",
        "import subprocess\n",
        "import io\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from sam2.build_sam import build_sam2\n",
        "from sam2.sam2_image_predictor import SAM2ImagePredictor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzsVKGSrg7b1"
      },
      "outputs": [],
      "source": [
        "torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16).__enter__()\n",
        "\n",
        "if torch.cuda.get_device_properties(0).major >= 8:\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFwZ6q-kg7b1"
      },
      "outputs": [],
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "CHECKPOINT = f\"{HOME}/checkpoints/sam2_hiera_large+.pt\"\n",
        "CONFIG = \"sam2_hiera_l.yaml\"\n",
        "sam2_model = build_sam2(CONFIG, CHECKPOINT, device=DEVICE, apply_postprocessing=False)\n",
        "predictor = SAM2ImagePredictor(sam2_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j89d9syghNvs"
      },
      "outputs": [],
      "source": [
        "def show_mask(mask, ax, random_color=False, borders=True):\n",
        "    if random_color:\n",
        "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
        "    else:\n",
        "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
        "    h, w = mask.shape[-2:]\n",
        "    mask = mask.astype(np.uint8)\n",
        "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "    if borders:\n",
        "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "        contours = [cv2.approxPolyDP(contour, epsilon=0.01, closed=True) for contour in contours]\n",
        "        mask_image = cv2.drawContours(mask_image, contours, -1, (1, 1, 1, 0.5), thickness=2)\n",
        "    ax.imshow(mask_image)\n",
        "\n",
        "def show_masks(image, masks, scores, point_coords=None, box_coords=None, input_labels=None, borders=True):\n",
        "    for i, (mask, score) in enumerate(zip(masks, scores)):\n",
        "        plt.figure(figsize=(10, 10))\n",
        "        plt.imshow(image)\n",
        "        show_mask(mask, plt.gca(), borders=borders)\n",
        "        if point_coords is not None:\n",
        "            assert input_labels is not None\n",
        "            show_points(point_coords, input_labels, plt.gca())\n",
        "        if box_coords is not None:\n",
        "            show_box(box_coords, plt.gca())\n",
        "        if len(scores) > 1:\n",
        "            plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n",
        "        plt.axis('off')\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dcg0PPnehPxL"
      },
      "outputs": [],
      "source": [
        "# Set up ngrok\n",
        "def start_ngrok():\n",
        "    url = ngrok.connect(8000)\n",
        "    print(\"ngrok tunnel URL is:\", url)\n",
        "    print(f\"ngrok tunnel available at: {url}\")\n",
        "\n",
        "# Functions for image processing and segmentation\n",
        "# Constants for resizing\n",
        "MAX_WIDTH = 1024\n",
        "MAX_HEIGHT = 768\n",
        "\n",
        "def adjust_points(points, resize_ratio):\n",
        "    adjusted_points = []\n",
        "    for point in points:\n",
        "        x = int(point['x'] * resize_ratio[0])\n",
        "        y = int(point['y'] * resize_ratio[1])\n",
        "        adjusted_points.append({'x': x, 'y': y, 'label': point.get('label', 0)})\n",
        "    return adjusted_points\n",
        "\n",
        "\n",
        "def resize_image(image, max_width, max_height):\n",
        "    h, w = image.shape[:2]\n",
        "    aspect_ratio = w / h\n",
        "    if w > max_width:\n",
        "        w = max_width\n",
        "        h = int(w / aspect_ratio)\n",
        "    if h > max_height:\n",
        "        h = max_height\n",
        "        w = int(h * aspect_ratio)\n",
        "    return cv2.resize(image, (w, h))\n",
        "\n",
        "\n",
        "def post_process_segmentation(image):\n",
        "    return cv2.GaussianBlur(image, (5, 5), 0)\n",
        "\n",
        "def get_image_from_base64(base64_str):\n",
        "    try:\n",
        "        encoded_data = base64_str.split(',')[1]\n",
        "        nparr = np.frombuffer(base64.b64decode(encoded_data), np.uint8)\n",
        "        img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
        "        if img is None:\n",
        "            raise ValueError(\"Image decode failed\")\n",
        "        return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
        "    except Exception as e:\n",
        "        print(f\"Error decoding image: {e}\")\n",
        "        return None\n",
        "\n",
        "def get_base64_encoded_image(image_path):\n",
        "    try:\n",
        "        with open(image_path, \"rb\") as img_file:\n",
        "            return base64.b64encode(img_file.read()).decode('utf-8')\n",
        "    except Exception as e:\n",
        "        print(f\"Error encoding image: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def annotate_image(image, masks, mode='single', opacity=0.6):\n",
        "    annotated_image = image.copy()\n",
        "\n",
        "    if mode == 'multiple':\n",
        "        # Generate a list of random colors, one for each mask\n",
        "        colors = [\n",
        "            [random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)]\n",
        "            for _ in range(len(masks))\n",
        "        ]\n",
        "        for mask, color in zip(masks, colors):\n",
        "            mask = mask.astype(bool)  # Ensure the mask is boolean\n",
        "            # Blend the color with the original image using the specified opacity\n",
        "            annotated_image[mask] = (\n",
        "                opacity * np.array(color, dtype=np.uint8) + (1 - opacity) * annotated_image[mask]\n",
        "            ).astype(np.uint8)\n",
        "    else:\n",
        "        # Single object mode: Use green color\n",
        "        color = np.array([0, 255, 0], dtype=np.uint8)  # Green color for the mask\n",
        "        for mask in masks:\n",
        "            mask = mask.astype(bool)  # Ensure the mask is boolean\n",
        "            # Blend the green color with the original image using the specified opacity\n",
        "            annotated_image[mask] = (\n",
        "                opacity * color + (1 - opacity) * annotated_image[mask]\n",
        "            ).astype(np.uint8)\n",
        "\n",
        "    return annotated_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3fuhGU4hQxF",
        "outputId": "bbc47c1b-d9d2-419a-a767-6806a473e245"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<flask_cors.extension.CORS at 0x7bdfbb523310>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "app = Flask(__name__)\n",
        "CORS(app, supports_credentials=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQKcqss3hTXc",
        "outputId": "fb2d9d5f-c3ec-4cc0-8475-1b8535dd6924"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * ngrok tunnel \"NgrokTunnel: \"https://5840-34-143-164-181.ngrok-free.app\" -> \"http://localhost:8000\"\" -> \"http://127.0.0.1:8000\"\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:8000\n",
            " * Running on http://172.28.0.12:8000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [31/Aug/2024 20:55:27] \"OPTIONS /segment HTTP/1.1\" 200 -\n",
            "/content/drive/MyDrive/segment-anything-2/sam2/modeling/sam/transformer.py:270: UserWarning: Memory efficient kernel not used because: (Triggered internally at ../aten/src/ATen/native/transformers/cuda/sdp_utils.cpp:718.)\n",
            "  out = F.scaled_dot_product_attention(q, k, v, dropout_p=dropout_p)\n",
            "/content/drive/MyDrive/segment-anything-2/sam2/modeling/sam/transformer.py:270: UserWarning: Memory Efficient attention has been runtime disabled. (Triggered internally at ../aten/src/ATen/native/transformers/sdp_utils_cpp.h:495.)\n",
            "  out = F.scaled_dot_product_attention(q, k, v, dropout_p=dropout_p)\n",
            "/content/drive/MyDrive/segment-anything-2/sam2/modeling/sam/transformer.py:270: UserWarning: Flash attention kernel not used because: (Triggered internally at ../aten/src/ATen/native/transformers/cuda/sdp_utils.cpp:720.)\n",
            "  out = F.scaled_dot_product_attention(q, k, v, dropout_p=dropout_p)\n",
            "/content/drive/MyDrive/segment-anything-2/sam2/modeling/sam/transformer.py:270: UserWarning: Expected query, key and value to all be of dtype: {Half, BFloat16}. Got Query dtype: float, Key dtype: float, and Value dtype: float instead. (Triggered internally at ../aten/src/ATen/native/transformers/sdp_utils_cpp.h:98.)\n",
            "  out = F.scaled_dot_product_attention(q, k, v, dropout_p=dropout_p)\n",
            "/content/drive/MyDrive/segment-anything-2/sam2/modeling/sam/transformer.py:270: UserWarning: CuDNN attention kernel not used because: (Triggered internally at ../aten/src/ATen/native/transformers/cuda/sdp_utils.cpp:722.)\n",
            "  out = F.scaled_dot_product_attention(q, k, v, dropout_p=dropout_p)\n",
            "/content/drive/MyDrive/segment-anything-2/sam2/modeling/sam/transformer.py:270: UserWarning: The CuDNN backend needs to be enabled by setting the enviornment variable`TORCH_CUDNN_SDPA_ENABLED=1` (Triggered internally at ../aten/src/ATen/native/transformers/cuda/sdp_utils.cpp:496.)\n",
            "  out = F.scaled_dot_product_attention(q, k, v, dropout_p=dropout_p)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562: UserWarning: Flash Attention kernel failed due to: No available kernel. Aborting execution.\n",
            "Falling back to all available kernels for scaled_dot_product_attention (which may have a slower speed).\n",
            "  return forward_call(*args, **kwargs)\n",
            "INFO:werkzeug:127.0.0.1 - - [31/Aug/2024 20:55:35] \"POST /segment HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [31/Aug/2024 20:55:46] \"OPTIONS /segment HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [31/Aug/2024 20:55:48] \"POST /segment HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [31/Aug/2024 20:56:08] \"OPTIONS /segment HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [31/Aug/2024 20:56:10] \"POST /segment HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [31/Aug/2024 21:05:10] \"OPTIONS /segment HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [31/Aug/2024 21:05:12] \"POST /segment HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [31/Aug/2024 21:08:16] \"OPTIONS /segment HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [31/Aug/2024 21:08:18] \"POST /segment HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [31/Aug/2024 21:08:42] \"OPTIONS /segment HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [31/Aug/2024 21:08:44] \"POST /segment HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [31/Aug/2024 21:09:14] \"OPTIONS /segment HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [31/Aug/2024 21:09:16] \"POST /segment HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [31/Aug/2024 21:09:43] \"OPTIONS /segment HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [31/Aug/2024 21:09:45] \"POST /segment HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [31/Aug/2024 21:10:46] \"OPTIONS /segment HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [31/Aug/2024 21:10:48] \"POST /segment HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [31/Aug/2024 21:11:44] \"OPTIONS /segment HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [31/Aug/2024 21:11:45] \"POST /segment HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [31/Aug/2024 21:13:17] \"OPTIONS /segment HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [31/Aug/2024 21:13:19] \"POST /segment HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [31/Aug/2024 21:14:41] \"OPTIONS /segment HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [31/Aug/2024 21:14:42] \"POST /segment HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [31/Aug/2024 21:16:14] \"OPTIONS /segment HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [31/Aug/2024 21:16:16] \"POST /segment HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [31/Aug/2024 21:18:08] \"OPTIONS /segment HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [31/Aug/2024 21:18:10] \"POST /segment HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [31/Aug/2024 21:19:33] \"OPTIONS /segment HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [31/Aug/2024 21:19:35] \"POST /segment HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [31/Aug/2024 21:21:10] \"OPTIONS /segment HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [31/Aug/2024 21:21:12] \"POST /segment HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [31/Aug/2024 21:23:14] \"OPTIONS /segment HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [31/Aug/2024 21:23:16] \"POST /segment HTTP/1.1\" 200 -\n"
          ]
        }
      ],
      "source": [
        "@app.route('/segment', methods=['POST'])\n",
        "def segment_image():\n",
        "    try:\n",
        "        data = request.get_json()\n",
        "        image_base64 = data['imageData']\n",
        "        points = data['points']\n",
        "        mode = data.get('mode', 'single')  # Default to single object\n",
        "\n",
        "        if not points or not isinstance(points, list):\n",
        "            return jsonify({'error': 'Invalid points data'}), 400\n",
        "\n",
        "        image_array = get_image_from_base64(image_base64)\n",
        "        if image_array is None:\n",
        "            return jsonify({'error': 'Invalid image data'}), 400\n",
        "\n",
        "        # Resize image and get resize ratio\n",
        "        original_height, original_width = image_array.shape[:2]\n",
        "        resized_image = resize_image(image_array, MAX_WIDTH, MAX_HEIGHT)\n",
        "        resized_height, resized_width = resized_image.shape[:2]\n",
        "        resize_ratio = (resized_width / original_width, resized_height / original_height)\n",
        "\n",
        "        temp_images_dir = os.path.join(os.getcwd(), 'temp_images')\n",
        "        if not os.path.exists(temp_images_dir):\n",
        "            os.makedirs(temp_images_dir)\n",
        "        image_path = os.path.join(temp_images_dir, 'temp_image.jpg')\n",
        "\n",
        "        cv2.imwrite(image_path, cv2.cvtColor(image_array, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "        # Adjust points coordinates\n",
        "        adjusted_points = adjust_points(points, resize_ratio)\n",
        "\n",
        "        valid_points = []\n",
        "        labels = []\n",
        "        for point in adjusted_points:\n",
        "            try:\n",
        "                x, y, label = int(point['x']), int(point['y']), int(point['label'])\n",
        "                if 0 <= x < resized_image.shape[1] and 0 <= y < resized_image.shape[0]:\n",
        "                    valid_points.append((x, y))\n",
        "                    labels.append(label)\n",
        "            except KeyError as e:\n",
        "                print(f\"Error accessing point: {point}, error: {e}\")\n",
        "\n",
        "        input_point = np.array(valid_points)\n",
        "        input_label = np.array(labels)\n",
        "\n",
        "        if mode == 'multiple':\n",
        "            # Convert the points and labels to the required format\n",
        "            pts_batch = [np.array(input_point).reshape(-1, 1, 2).astype(np.float32)]\n",
        "            labels_batch = [np.ones((len(input_point), 1), dtype=np.int32)]  # Assuming label 1 for all points\n",
        "\n",
        "            img_batch = [resized_image]\n",
        "            predictor.set_image_batch(img_batch)\n",
        "\n",
        "            # Predict masks and scores\n",
        "            masks_batch, scores_batch, _ = predictor.predict_batch(\n",
        "                pts_batch, labels_batch,\n",
        "                box_batch=None,\n",
        "                multimask_output=True\n",
        "            )\n",
        "\n",
        "            # Select the best single mask per object\n",
        "            best_masks = []\n",
        "            for masks, scores in zip(masks_batch, scores_batch):\n",
        "                best_masks.append(masks[np.arange(len(masks)), np.argmax(scores, axis=-1)])\n",
        "\n",
        "            masks = [mask for sublist in best_masks for mask in sublist]\n",
        "\n",
        "        else:\n",
        "            predictor.set_image(resized_image)\n",
        "            # Single object segmentation\n",
        "            masks, scores, logits = predictor.predict(\n",
        "                point_coords=input_point,\n",
        "                point_labels=input_label,\n",
        "                multimask_output=True,\n",
        "            )\n",
        "            mask_input = logits[np.argmax(scores), :, :]\n",
        "            masks, scores, _ = predictor.predict(\n",
        "                point_coords=input_point,\n",
        "                point_labels=input_label,\n",
        "                mask_input=mask_input[None, :, :],\n",
        "                multimask_output=False,\n",
        "            )\n",
        "\n",
        "        if len(masks) > 0:\n",
        "            # Pass mode parameter to annotate_image\n",
        "            mask_annotated_image = annotate_image(resized_image, masks, mode)\n",
        "            mask_image_path = os.path.join(temp_images_dir, '1.jpeg')\n",
        "            cv2.imwrite(mask_image_path, cv2.cvtColor(mask_annotated_image, cv2.COLOR_RGB2BGR))\n",
        "            mask_image = post_process_segmentation(mask_annotated_image)\n",
        "            result_base64 = get_base64_encoded_image(mask_image_path)\n",
        "\n",
        "            return jsonify({'status': 'success', 'segmentedImage': result_base64})\n",
        "        else:\n",
        "            return jsonify({'status': 'success', 'segmentedImage': None})\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during processing: {e}\")\n",
        "        traceback.print_exc()\n",
        "        return jsonify({'error': 'Server error', 'message': str(e)}), 500\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    ngrok.set_auth_token(\"2l6r6vCbENywTecJF2Hwoe8Wqxh_6DC3QZbyZatFAMwowGx5i\")\n",
        "    public_url = ngrok.connect(8000)\n",
        "    print(f\" * ngrok tunnel \\\"{public_url}\\\" -> \\\"http://127.0.0.1:8000\\\"\")\n",
        "    app.run(host='0.0.0.0', port=8000)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7Ce0zS9xJ_Gu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}